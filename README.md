# Treinamento de Rede Neural
![ezgif com-gif-maker](https://user-images.githubusercontent.com/67600860/136825544-05c9b3d4-d5ce-4e81-a4c9-0b30acb13628.gif)

<p align="justify">
üíª Como trabalho proposto na disciplina Redes Neurais Artificiais da gradua√ß√£o em Engenharia El√©trica, este √© um c√≥digo b√°sico que implementa, passo a passo, o processo de treinamento de uma Perceptron de M√∫ltiplas Camadas por meio do algoritmo backpropagtion. Para isso, foi utilizado um banco de dados conhecido para problemas de classifica√ß√£o, o dataset <a href="https://archive.ics.uci.edu/ml/datasets/iris">Iris</a> üåº
</p>
  
OBS: O c√≥digo tem por objetivo exemplificar a etapa de treino üèãüèæ‚Äç‚ôÄ e atualiza√ß√£o dos pesos, as outras etapas importnates como valida√ß√£o cruzada e teste n√£o est√£o presentes neste c√≥digo.

# Modelo de Neur√¥nio
<p align="justify">
O neur√¥nio √© a unidade b√°sica morfofuncional do tecido nervoso e, da
mesma maneira, √© a unidade b√°sica de processamento da rede neural. Estes
modelos computacionais inspirados em neur√¥nios biol√≥gicos possuem
algumas caracter√≠sticas semelhantes aos naturais para poder desfrutar da
capacidade de aprendizado, s√£o elas: sinais de entrada, pesos sin√°pticos,
fun√ß√£o de soma, fun√ß√£o de ativa√ß√£o e sinal de sa√≠da.
Para realizar esse processo em uma linguagem de programa√ß√£o, √©
aconselh√°vel utilizar opera√ß√µes matriciais, pois assim ocorre uma otimiza√ß√£o
na etapa de compila√ß√£o. Teremos ent√£o, um vetor de entrada e um vetor de
pesos (ou matriz, caso haja mais de um neur√¥nio na camada).
</p>

![neuronio](https://user-images.githubusercontent.com/67600860/136829925-bfd180e6-f59b-4901-85d3-a3c2a8d2a98b.png)

# Forma de Aprendizado
<p align="justify">
Os pesos sin√°pticos s√£o pe√ßas fundamentais para o armazenamento de
informa√ß√£o do neur√¥nio artificial, √© atrav√©s deles que o modelo possui a
capacidade de aprender. Utiliza-se o algoritmo de Backpropagation,
que possibilita o ajuste dos pesos em camadas intermedi√°rias, fazendo a
chamada ‚Äúretropropaga√ß√£o do erro‚Äù, ou seja, utiliza o valor do erro da
camada de sa√≠da no ajuste dos pesos das outras camadas ocultas.
 </p>

# Algoritmo Backpropagation
<p align="justify">
A inicializa√ß√£o dos pesos sin√°pticos ocorre de maneira rand√¥mica, a
escolha dos pesos iniciais pode prevenir o problema do m√≠nimo local e ainda
a satura√ß√£o prematura da rede. O fluxo de informa√ß√£o acontece em duas
etapas, primeiramente na etapa "forward", a rede √© apresentada aos padr√µes
de treinamento e tem seus pesos excitados a produzir uma sa√≠da, na etapa
‚Äúbackward‚Äù, calcula-se o erro da rede para aquele padr√£o e este erro, ent√£o, √©
"retro propagado" para as outras camadas ocultas da rede, que n√£o tem
acesso direto ao erro da camada de sa√≠da. √â nesta etapa que os pesos s√£o atualizados e a rede adquire conhecimento, o processo se repete at√© que o
crit√©rio de parada seja satisfeito (o crit√©rio adotado foi o de n√∫mero de
√©pocas de treinamento).
</p>

**A imagem abaixo faz um resumo geral das etapas do c√≥digo**

<p align="center">
  <img src="https://user-images.githubusercontent.com/67600860/136827442-aff9e61d-ec4c-437f-98d6-1b8e20688ff6.png" />
</p>

# Gr√°ficos
*Os gr√°ficos apresentados a seguir tem por finalidade melhorar a compreens√£o dos processos executados e a import√¢ncia de cada um. Para gerar estes gr√°ficos, utilizou-se 5 neur√¥nios na camada escondida, 300 √©pocas e taxa de aprendizado de 0.01*

* Imagem 1 - Evolu√ß√£o do erro por n√∫mero de √©pocas

<p align="center">
  <img src="https://user-images.githubusercontent.com/67600860/136830197-fa51fda2-5f17-40d4-9f6f-7149d4c4f9ec.jpg" />
</p>
<p align="justify">
Esta imagem mostra a magnitude do erro quadr√°tico m√©dio para cada n√∫mero de √©pocas, onde o erro se inicia em aproximadamente 0.04 e finaliza em aproximadamente 0.015 na √©poca de n√∫mero 300
</p>

* Imagem 2 - Sa√≠da desejada e sa√≠da de treinamento

<p align="center">
  <img src="https://user-images.githubusercontent.com/67600860/136830662-dc29ecb8-bbc2-4a29-956f-a7a4e7a25b9a.jpg" />
</p>

<p align="justify">
Apresenta os valores de sa√≠da do treinamento (c√≠rulo azul) sobrepostos aos valores de sa√≠da desejado (asterisco verde) para as classes 1 (Iris Setosa), 2 (Iris
Versicolour) e 3 (Iris Virginica). Note que esta imagem n√£o representa a classifica√ß√£o final, j√° que os valores est√£o plotados de maneira cont√≠nua. Para performar a classifica√ß√£o, poder√≠amos por exemplo instituir limites de valores para as classes, sendo a classe 1 correspondendo aos valores de sa√≠da entre 0.5 e 1.5, a classe 2 aos valores de 1.5 a 2.5 e a classe 3 aos valores entre 2.5 e 3.5
 </p>

* Imagem 3 - Sa√≠das e √©pocas

<p align="center">
  <img src="https://user-images.githubusercontent.com/67600860/136830937-b0bf0b76-0ec7-4e6d-b08b-50e3bf7ea43b.jpg" />
</p>

<p align="justify">
A finalizade desta imagem √© proporcionar o "feeling" sobre como as classifica√ß√µes v√£o se ajustando e ficando melhor √† medida que o n√∫mero de √©pocas aumenta. Na imagem podemos observar que as classifica√ß√µes come√ßam muito dispersas e se concentram √† medida que os pesos se ajustam
 </p>
 
 
 ***Para qualquer d√∫vida, considera√ß√£o ou sugest√µes, fique √† vontade de entrar em contato üìß***

